{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\enire\\anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import spacy.attrs\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import gensim.downloader as api\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd = pd.read_csv(\"C19_measures.csv\") #for extracting # of COVID words analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pd.read_csv(\"C19_for_embeddings.csv\")  #for extracting text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd1 = pd[(pd[\"covid_words\"]>0) | (pd[\"subreddit\"].str.contains(\"covid\", case=False)) | (pd[\"subreddit\"].str.contains(\"vaccine\", case=False))]\n",
    "pd1 = pd1[pd1[\"afterCOVID\"]==0]\n",
    "pd1.to_csv(\"beforeCOVIDdata.csv\", index = False)\n",
    "\n",
    "pd2= pd[(pd[\"covid_words\"]>0) | (pd[\"subreddit\"].str.contains(\"covid\", case=False)) | (pd[\"subreddit\"].str.contains(\"vaccine\", case=False))]\n",
    "pd2 = pd2[pd2[\"afterCOVID\"]==1]\n",
    "pd2.to_csv(\"afterCOVIDdata.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_filter = pd.DataFrame()   #creating new data frame for beforecovid posts w/ text & save in csv\n",
    "for id in pd1.index:\n",
    "    print(id)\n",
    "    df3 = {\"User_Name\": text.iloc[id,0],\"created_utc\": text.iloc[id,1],\"afterCOVID\": 0, \"subreddit\": pd1.loc[id,\"subreddit\"],\"covid_words\":pd1.loc[id,\"covid_words\"] ,\"sentiment\":pd1.loc[id,\"sentiment\"],\"text\":text.iloc[id,3]}\n",
    "    before_filter = before_filter.append(df3, ignore_index = True)\n",
    "    \n",
    "before_filter.to_csv(\"before_filter.csv\", index = False)\n",
    "before_filter = pd.read_csv(\"before_filter.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_filter = pd.DataFrame()  #creating new data frame for aftercovid posts w/ text & save in csv\n",
    "for id in pd2.index:\n",
    "    print(id)\n",
    "    df3 = {\"User_Name\": text.iloc[id,0],\"created_utc\": text.iloc[id,1],\"afterCOVID\": 1, \"subreddit\": pd2.loc[id,\"subreddit\"],\"covid_words\":pd2.loc[id,\"covid_words\"] ,\"sentiment\":pd2.loc[id,\"sentiment\"],\"text\":text.iloc[id,3]}\n",
    "    after_filter = after_filter.append(df3, ignore_index = True)\n",
    "    \n",
    "after_filter.to_csv(\"after_filter.csv\", index = False)\n",
    "after_filter = pd.read_csv(\"after_filter.csv\")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the csv \n",
    "before_filter = pd.read_csv(\"before_filter.csv\")  \n",
    "after_filter = pd.read_csv(\"after_filter.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling\n",
    "before_filter_sample = before_filter[1:5000]\n",
    "after_filter_sample = after_filter[1:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_filter_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load spacey nlp\n",
    "verbose = False\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "nlp.max_length = 2000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering purposes\n",
    "punctuation = [\".\", \"!\", \"?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove emoji & amp\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\U0001f919\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               u\"\\u200b\"\n",
    "                               u\"\\u2010\"\n",
    "                               u\"\\u03b1\"\n",
    "                               u\"\\u2070-\\u2260\"\n",
    "                               u\"\\u0131\"\n",
    "                               u\"\\u011f-\\u015f\"\n",
    "                               u\"\\u202f\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_text = \"\"     \n",
    "column = before_filter_sample.text\n",
    "for each in column:\n",
    "    if each[-1] not in punctuation:\n",
    "        each = each +\".\"\n",
    "    before_text = before_text + \" \" + each\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_text = \"\"\n",
    "column = after_filter_sample.text\n",
    "for each in column:\n",
    "    if each[-1] not in punctuation:\n",
    "        each = each +\".\"\n",
    "    after_text = after_text + \" \" + each\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_text = remove_emoji(before_text.lower())\n",
    "after_text = remove_emoji(after_text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save purposes \n",
    "with open(\"before_text.txt\", \"w\") as f:\n",
    "    f.write(before_text)\n",
    "    \n",
    "with open(\"after_text.txt\",\"w\" ) as f:\n",
    "    f.write(after_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open saved file\n",
    "with open(\"before_text.txt\", \"r\") as f:\n",
    "    before_text1 = f.read()\n",
    "\n",
    "with open(\"after_text.txt\", \"r\") as f:\n",
    "    after_text1 = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the w2vdoc models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vdoc = nlp(before_text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentlist = [sent.text.strip(\". !?\") for sent in w2vdoc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vdoc2 = nlp(after_text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentlist = [sent.text.strip(\". !?\") for sent in w2vdoc2.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Algorithm to create a list of tokenized sentences\"\"\"\n",
    "listolists = []\n",
    "for each in sentlist:\n",
    "    thissent = []\n",
    "    hold = nlp(each)\n",
    "    for eachtoken in hold:\n",
    "        if eachtoken.is_space or eachtoken.is_stop:\n",
    "            continue\n",
    "        else:\n",
    "            eachtoken = eachtoken.lemma_  #convert to lemma form bc we have a small data set\n",
    "            eachtoken = str(eachtoken)\n",
    "            thissent.append(eachtoken)\n",
    "    if thissent != []:\n",
    "        listolists.append(thissent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Run word2vec to create a w2v model\"\"\"\n",
    "model = Word2Vec(listolists, min_count=2)\n",
    "model.wv.save_word2vec_format('BinaryLandbackModel.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Word2Vec(listolists, min_count=2)\n",
    "model2.wv.save_word2vec_format('BinaryLandbackModel2.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Word2Vec.load('BinaryLandbackModel.bin')\n",
    "#model2 = Word2Vec.load('BinaryLandBackModel2.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before COVID:\n",
      "Top 25 words similar to 'covid':\n",
      "(1):  doctor------0.9951851963996887\n",
      "(2):  baby------0.9941032528877258\n",
      "(3):  tell------0.9940159916877747\n",
      "(4):  need------0.9925158619880676\n",
      "(5):  true------0.9923438429832458\n",
      "(6):  yeah------0.9922130107879639\n",
      "(7):  know------0.9915797710418701\n",
      "(8):  mean------0.9913991689682007\n",
      "(9):  accord------0.9913396239280701\n",
      "(10):  ward------0.9906923174858093\n",
      "(11):  number------0.9906181693077087\n",
      "(12):  not------0.9904426336288452\n",
      "(13):  plant------0.9902167916297913\n",
      "(14):  guess------0.9901776909828186\n",
      "(15):  case------0.9901561737060547\n",
      "(16):  consider------0.9901284575462341\n",
      "(17):  right------0.990083634853363\n",
      "(18):  responsible------0.9898399710655212\n",
      "(19):  flu------0.9894385933876038\n",
      "(20):  precaution------0.9893040657043457\n",
      "(21):  delay------0.9892234206199646\n",
      "(22):  guy------0.9891847968101501\n",
      "(23):  scenario------0.9891186952590942\n",
      "(24):  google------0.9888888001441956\n",
      "(25):  immunity------0.9887418150901794\n",
      "After COVID:\n",
      "Top 25 words similar to 'covid':\n",
      "(1):  contagious------0.9962073564529419\n",
      "(2):  confirm------0.9961245059967041\n",
      "(3):  expose------0.9956574440002441\n",
      "(4):  case------0.9955943822860718\n",
      "(5):  asymptomatic------0.9955438375473022\n",
      "(6):  infection------0.9952675700187683\n",
      "(7):  result------0.9951648116111755\n",
      "(8):  got------0.995132327079773\n",
      "(9):  load------0.9951081275939941\n",
      "(10):  infectious------0.9950907230377197\n",
      "(11):  20------0.9948573708534241\n",
      "(12):  isolate------0.9947651624679565\n",
      "(13):  accurate------0.9947428107261658\n",
      "(14):  long------0.9946513175964355\n",
      "(15):  january------0.9945401549339294\n",
      "(16):  free------0.9944214820861816\n",
      "(17):  icu------0.9943399429321289\n",
      "(18):  likely------0.9942951798439026\n",
      "(19):  clinic------0.9942656755447388\n",
      "(20):  vax------0.994174599647522\n",
      "(21):  begin------0.9941478967666626\n",
      "(22):  testing------0.9941408634185791\n",
      "(23):  1st------0.9940490126609802\n",
      "(24):  longhaul------0.9940169453620911\n",
      "(25):  nasal------0.9939851760864258\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('contagious', 0.9962073564529419),\n",
       " ('confirm', 0.9961245059967041),\n",
       " ('expose', 0.9956574440002441),\n",
       " ('case', 0.9955943822860718),\n",
       " ('asymptomatic', 0.9955438375473022),\n",
       " ('infection', 0.9952675700187683),\n",
       " ('result', 0.9951648116111755),\n",
       " ('got', 0.995132327079773),\n",
       " ('load', 0.9951081275939941),\n",
       " ('infectious', 0.9950907230377197),\n",
       " ('20', 0.9948573708534241),\n",
       " ('isolate', 0.9947651624679565),\n",
       " ('accurate', 0.9947428107261658),\n",
       " ('long', 0.9946513175964355),\n",
       " ('january', 0.9945401549339294),\n",
       " ('free', 0.9944214820861816),\n",
       " ('icu', 0.9943399429321289),\n",
       " ('likely', 0.9942951798439026),\n",
       " ('clinic', 0.9942656755447388),\n",
       " ('vax', 0.994174599647522),\n",
       " ('begin', 0.9941478967666626),\n",
       " ('testing', 0.9941408634185791),\n",
       " ('1st', 0.9940490126609802),\n",
       " ('longhaul', 0.9940169453620911),\n",
       " ('nasal', 0.9939851760864258)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def GetCosSimList(model, word, verbose:bool=True):\n",
    "    \"\"\"Returns and can print a list of the words in our doc that are most\n",
    "    cosine-similar to a word the function is passed\"\"\"\n",
    "    word = word.lower()\n",
    "    simlist = model.wv.similar_by_vector(word, topn=25, restrict_vocab=None)\n",
    "    if verbose:\n",
    "        print(\"Top 25 words similar to '\" + word + \"':\")\n",
    "        ct = 1\n",
    "        for tup in simlist:\n",
    "            print(\"(\" + str(ct) + \"):  \" + tup[0] + \"------\" + str(tup[1]))\n",
    "            ct += 1\n",
    "    return simlist\n",
    "\n",
    "print(\"Before COVID:\")\n",
    "GetCosSimList(model, \"covid\")\n",
    "\n",
    "print(\"After COVID:\")\n",
    "GetCosSimList(model2, \"covid\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
